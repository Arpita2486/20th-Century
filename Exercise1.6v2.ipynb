{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5947a18e-ebfb-418d-8a7e-189bf38c6aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root : C:\\Users\\arpit\\Documents\\CareerFoundry\n",
      "Data path    : C:\\Users\\arpit\\Documents\\CareerFoundry\\key_events_20th_century_text.txt (exists: True)\n",
      "Output dir   : C:\\Users\\arpit\\Documents\\CareerFoundry\\outputs\\Exercise_1_6\n",
      "Loaded 66,741 characters.\n",
      "\n",
      "The 20th century changed the world in unprecedented ways. The World Wars sparked tension between countries and led to the creation of atomic bombs , the Cold War led to the Space Race and the creation of space-based rockets, and the World Wide Web was created. These advancements have played a significant role in citizens' lives and shaped the 21st century into what it is today.\n",
      "The new beginning of the 20th century marked significant changes. The 1900s saw the decade herald a series of inventions, including the automobile , airplane and radio broadcasting . 1914 saw the completion of the Panama Canal .\n",
      "The Scramble for Africa continued in the 1900s and resulted in wars and genocide across the continent. The atrocities in the Congo Free State shocked the uncolonised world.\n",
      "From 1914 to 1918\n",
      "...\n",
      "Top non-ASCII chars BEFORE cleaning: [('–', 4), ('é', 3), ('ö', 3), ('í', 2), ('\\xa0', 2), ('—', 1), ('ã', 1), ('’', 1)]\n",
      "Top non-ASCII chars AFTER  cleaning: [('é', 3), ('ö', 3), ('í', 2), ('ã', 1)]\n",
      "\n",
      "Saved cleaned text to: C:\\Users\\arpit\\Documents\\CareerFoundry\\outputs\\Exercise_1_6\\key_events_20th_century_text_clean.txt\n",
      "Using built-in canonical list: 68 countries.\n",
      "Document length (tokens): 12896\n",
      "Sample entities: [('The 20th century', 'DATE'), ('the Cold War', 'EVENT'), ('the Space Race', 'WORK_OF_ART'), ('the World Wide Web', 'WORK_OF_ART'), ('the 21st century', 'DATE'), ('today', 'DATE'), ('the 20th century', 'DATE'), ('The 1900s', 'DATE'), ('the decade', 'DATE'), ('the Panama Canal', 'ORG'), ('Scramble', 'PRODUCT'), ('Africa', 'LOC'), ('the 1900s', 'DATE'), ('the Congo Free State', 'FAC'), ('1914 to 1918', 'DATE')]\n",
      "Sentences with (GPE/LOC) entities: 222\n",
      "Example: ('The Scramble for Africa continued in the 1900s and resulted in wars and genocide across the continent.', ['Africa'])\n",
      "Sentences with recognized countries (canonical): 130\n",
      "Example: ('After a period of diplomatic and military escalation known as the July Crisis , by the end of July 1914 two coalitions were at war: the Allies, comprised initially of the British Empire , France , and the Russian Empire ; and the Central Powers , comprised initially of the German Empire and Austria-Hungary .', ['Austria', 'France', 'Germany'])\n",
      "Total raw pairs: 111\n",
      "Saved relationships to: C:\\Users\\arpit\\Documents\\CareerFoundry\\outputs\\Exercise_1_6\\country_relationships.csv (rows: 81)\n"
     ]
    }
   ],
   "source": [
    "import os, re, unicodedata, itertools, collections\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# spaCy for NER\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    print(\"spaCy model 'en_core_web_sm' not found. Install with:\\n  python -m spacy download en_core_web_sm\")\n",
    "    import spacy\n",
    "    nlp = None\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "DATA_PATH = PROJECT_ROOT / \"key_events_20th_century_text.txt\"   # adjust if needed\n",
    "OUT_DIR = PROJECT_ROOT / \"outputs\" / \"Exercise_1_6\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root : {PROJECT_ROOT}\")\n",
    "print(f\"Data path    : {DATA_PATH} (exists: {DATA_PATH.exists()})\")\n",
    "print(f\"Output dir   : {OUT_DIR}\")\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(\"Could not find 'key_events_20th_century_text.txt' at project root. Update DATA_PATH if needed.\")\n",
    "\n",
    "raw_text = DATA_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "print(f\"Loaded {len(raw_text):,} characters.\\n\")\n",
    "print(raw_text[:800] + \"\\n...\")\n",
    "\n",
    "def non_ascii_report(text, top_n=25):\n",
    "    non_ascii = [c for c in text if ord(c) > 127]\n",
    "    counts = collections.Counter(non_ascii).most_common(top_n)\n",
    "    return counts\n",
    "\n",
    "def basic_clean(text: str) -> str:\n",
    "    # Unicode normalization\n",
    "    x = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Replace common unicode punctuation with ASCII equivalents\n",
    "    replacements = {\n",
    "        \"\\u2018\": \"'\", \"\\u2019\": \"'\", \"\\u201C\": '\"', \"\\u201D\": '\"',\n",
    "        \"\\u2013\": \"-\", \"\\u2014\": \"-\", \"\\u00A0\": \" \", \"\\u2026\": \"...\",\n",
    "    }\n",
    "    for k, v in replacements.items():\n",
    "        x = x.replace(k, v)\n",
    "\n",
    "    # Collapse whitespace\n",
    "    x = re.sub(r\"[ \\t]+\", \" \", x)\n",
    "    x = re.sub(r\"\\s+\\n\", \"\\n\", x)\n",
    "    x = re.sub(r\"\\n{3,}\", \"\\n\\n\", x).strip()\n",
    "    return x\n",
    "\n",
    "print(\"Top non-ASCII chars BEFORE cleaning:\", non_ascii_report(raw_text))\n",
    "clean_text = basic_clean(raw_text)\n",
    "print(\"Top non-ASCII chars AFTER  cleaning:\", non_ascii_report(clean_text))\n",
    "\n",
    "CLEAN_PATH = OUT_DIR / \"key_events_20th_century_text_clean.txt\"\n",
    "CLEAN_PATH.write_text(clean_text, encoding=\"utf-8\")\n",
    "print(f\"\\nSaved cleaned text to: {CLEAN_PATH}\")\n",
    "\n",
    "user_list = None\n",
    "for fn in [\"countries_list.txt\", \"countries.txt\"]:\n",
    "    p = Path(fn)\n",
    "    if p.exists():\n",
    "        try:\n",
    "            user_list = [line.strip() for line in p.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines() if line.strip()]\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {fn}: {e}\")\n",
    "\n",
    "if user_list:\n",
    "    CANON_COUNTRIES = sorted(set(user_list))\n",
    "    print(f\"Loaded {len(CANON_COUNTRIES)} countries from your file.\")\n",
    "else:\n",
    "    CANON_COUNTRIES = sorted({\n",
    "        # Major & frequently-mentioned (edit as needed)\n",
    "        \"United States\",\"United Kingdom\",\"France\",\"Germany\",\"Italy\",\"Spain\",\"Portugal\",\n",
    "        \"Russia\",\"Soviet Union\",\"China\",\"Japan\",\"India\",\"Pakistan\",\"Canada\",\"Australia\",\n",
    "        \"Poland\",\"Czechoslovakia\",\"Yugoslavia\",\"Austria\",\"Hungary\",\"Netherlands\",\"Belgium\",\n",
    "        \"Switzerland\",\"Sweden\",\"Norway\",\"Denmark\",\"Finland\",\"Ireland\",\"Greece\",\"Turkey\",\n",
    "        \"Israel\",\"Palestine\",\"Egypt\",\"Iran\",\"Iraq\",\"Syria\",\"Lebanon\",\"Jordan\",\"Saudi Arabia\",\n",
    "        \"Korea\",\"North Korea\",\"South Korea\",\"Vietnam\",\"North Vietnam\",\"South Vietnam\",\"Taiwan\",\n",
    "        \"Czech Republic\",\"Slovakia\",\"East Germany\",\"West Germany\",\n",
    "        \"Brazil\",\"Argentina\",\"Mexico\",\"Cuba\",\"Chile\",\"Peru\",\"Colombia\",\n",
    "        \"South Africa\",\"Ethiopia\",\"Kenya\",\"Nigeria\",\"Ghana\",\"Algeria\",\"Morocco\",\n",
    "        # Historical empires/states that may appear\n",
    "        \"Ottoman Empire\",\"Austro-Hungarian Empire\",\"Russian Empire\",\"Prussia\"\n",
    "    })\n",
    "    print(f\"Using built-in canonical list: {len(CANON_COUNTRIES)} countries.\")\n",
    "\n",
    "# Synonyms/variants → canonical\n",
    "COUNTRY_SYNONYMS = {\n",
    "    # US/UK\n",
    "    \"u.s.\":\"United States\",\"u.s\":\"United States\",\"u.s.a.\":\"United States\",\"usa\":\"United States\",\"us\":\"United States\",\"america\":\"United States\",\n",
    "    \"united states of america\":\"United States\",\n",
    "    \"u.k.\":\"United Kingdom\",\"uk\":\"United Kingdom\",\"great britain\":\"United Kingdom\",\"britain\":\"United Kingdom\",\"england\":\"United Kingdom\",\n",
    "    # Russia/USSR\n",
    "    \"ussr\":\"Soviet Union\",\"soviet russia\":\"Soviet Union\",\"russian empire\":\"Russian Empire\",\n",
    "    # Germany\n",
    "    \"german empire\":\"Germany\",\"prussian empire\":\"Prussia\",\"west germany\":\"West Germany\",\"east germany\":\"East Germany\",\n",
    "    # China/Taiwan\n",
    "    \"prc\":\"China\",\"people's republic of china\":\"China\",\"peoples republic of china\":\"China\",\"republic of china\":\"Taiwan\",\"roc\":\"Taiwan\",\n",
    "    # Korea/Vietnam\n",
    "    \"korean peninsula\":\"Korea\",\"dprk\":\"North Korea\",\"rok\":\"South Korea\",\n",
    "    \"democratic republic of vietnam\":\"North Vietnam\",\"republic of vietnam\":\"South Vietnam\",\n",
    "    # Other common variants\n",
    "    \"czechia\":\"Czech Republic\",\"holland\":\"Netherlands\",\"ivory coast\":\"Ivory Coast\"\n",
    "}\n",
    "\n",
    "def canonicalize_country(name: str):\n",
    "    \"\"\"Map a raw entity string to a canonical country name if possible.\"\"\"\n",
    "    if not name:\n",
    "        return None\n",
    "    s = re.sub(r\"[^\\w\\s\\-&\\.']\", \" \", name.lower()).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    if s in COUNTRY_SYNONYMS:\n",
    "        return COUNTRY_SYNONYMS[s]\n",
    "    # Title-case fallback (e.g., \"France\", \"Soviet Union\")\n",
    "    t = re.sub(r\"\\s+\", \" \", name.strip())\n",
    "    return t\n",
    "\n",
    "if nlp is None:\n",
    "    raise RuntimeError(\"spaCy language model not loaded. Run: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "doc = nlp(clean_text)\n",
    "print(f\"Document length (tokens): {len(doc)}\")\n",
    "print(\"Sample entities:\", [(ent.text, ent.label_) for ent in doc.ents[:15]])\n",
    "\n",
    "# Collect country-like entities per sentence using spaCy labels\n",
    "sentence_entities = []   # list of tuples: (sentence_text, [entity_texts])\n",
    "for sent in doc.sents:\n",
    "    ents = [ent for ent in sent.ents if ent.label_ in (\"GPE\",\"LOC\")]\n",
    "    if ents:\n",
    "        sentence_entities.append((sent.text, [e.text for e in ents]))\n",
    "\n",
    "print(f\"Sentences with (GPE/LOC) entities: {len(sentence_entities)}\")\n",
    "print(\"Example:\", sentence_entities[0] if sentence_entities else \"No sentences with GPE/LOC found.\")\n",
    "\n",
    "\n",
    "filtered_sentences = []  # (sentence_text, [canonical_country_names])\n",
    "for sent_text, ents in sentence_entities:\n",
    "    canon = []\n",
    "    for e in ents:\n",
    "        c = canonicalize_country(e)\n",
    "        if c in CANON_COUNTRIES:\n",
    "            canon.append(c)\n",
    "    canon = sorted(set(canon))  # unique within sentence\n",
    "    if canon:\n",
    "        filtered_sentences.append((sent_text, canon))\n",
    "\n",
    "print(f\"Sentences with recognized countries (canonical): {len(filtered_sentences)}\")\n",
    "if filtered_sentences:\n",
    "    print(\"Example:\", filtered_sentences[0])\n",
    "\n",
    "\n",
    "pairs = []\n",
    "for _, countries in filtered_sentences:\n",
    "    if len(countries) >= 2:\n",
    "        for a, b in itertools.combinations(sorted(countries), 2):\n",
    "            pairs.append((a, b))\n",
    "\n",
    "print(f\"Total raw pairs: {len(pairs)}\")\n",
    "\n",
    "# Aggregate counts into a DataFrame\n",
    "if pairs:\n",
    "    df_pairs = pd.DataFrame(pairs, columns=[\"country1\",\"country2\"])\n",
    "    df_rel = df_pairs.value_counts().reset_index(name=\"weight\")\n",
    "else:\n",
    "    df_rel = pd.DataFrame(columns=[\"country1\",\"country2\",\"weight\"])\n",
    "\n",
    "df_rel.head()\n",
    "\n",
    "REL_CSV = OUT_DIR / \"country_relationships.csv\"\n",
    "df_rel.to_csv(REL_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved relationships to: {REL_CSV} (rows: {len(df_rel)})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c905fe-e56f-48d3-a82d-b5d790b5487a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "20c_nlp",
   "language": "python",
   "name": "20c_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
